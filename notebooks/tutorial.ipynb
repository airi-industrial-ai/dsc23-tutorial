{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOVdUCBHr7jM"
      },
      "source": [
        "# A tutorial on model validation using deep generation of stress data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zn5wD2gr7jP"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/airi_logo.png\" width=200 align='left'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTKo8EAwr7jQ"
      },
      "source": [
        "_Vitaliy Pozdnyakov, Junior Research Scientist | AIRI_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5SDfjqWr7jT"
      },
      "source": [
        "Open the notebook by the following QR code:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/qr_code.png\" width=200 align='left'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3UCt2TUr7jU"
      },
      "source": [
        "## Our plan for today"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW5uY0F6r7jU"
      },
      "source": [
        "1. Theory: Dataset shifts\n",
        "2. Practice: Download and prepare a dataset for time series forecasting\n",
        "3. Practice: Train and test a LSTM-based forecasting model\n",
        "4. Theory: Worst-case risk\n",
        "5. Practice: Evaluate the stability of the LSTM-based model using the worst-case risk\n",
        "6. Theory: Time series generation\n",
        "7. Practice: Generate fake time series similar to the original dataset\n",
        "8. Practice: Evaluate the stablity of the LSTM-based model using the worst-case risk on fake data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrFz2WZur7jU"
      },
      "source": [
        "## Dataset shifts in the real setting\n",
        "\n",
        "Let's consider some predictive model, say a recommender system. In the ideal world we expect the following process: we train the model on historical data, then we run it on actual data and see the great results in terms of some metrics (MSE, Cross-entropy, etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jejksC4hr7jV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/ideal_setting.png\" width=800 align='left'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA6EYMAdr7jV"
      },
      "source": [
        "**The main assumption**: train and test datasets are sampled from the same distributions or at least similar distributions.\n",
        "\n",
        "In reality, we often face some **challenges** in the real world setting:\n",
        "* data usually changes over time\n",
        "* shocks affect dramatically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9vptjkUr7jW"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/challenges.png\" width=450 align=\"left\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ3Cd2A1r7jW"
      },
      "source": [
        "### Obviously, models fail when¬†data is changed, do¬†not¬†they?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okqaW6wEr7jW"
      },
      "source": [
        "The problem of changed data is known as **dataset (distribution) shift**. We consider two types of distribution shifts: concept shifts and covariate shifts.\n",
        "\n",
        "**Concept shift** is the shift in the conditional distribution of target by given features while the distribution of features remains the same:\n",
        "\n",
        "$$P_{tr}(X) = P_{tst}(X)$$\n",
        "$$P_{tr}(y|X) \\neq P_{tst}(y|X)$$\n",
        "\n",
        "Example: ùëã is a message, ùë¶ is a¬†spam marker. We test the model on a new type of spam.\n",
        "\n",
        "In the case of the concept shift we actually need a new model. The following techinques are usually used: retraining, fine-tuning, online learning.\n",
        "\n",
        "**Covariate shift** is the shift in the distribution of features while the conditional distribution of target by given features remains the same:\n",
        "\n",
        "$$P_{tr}(X) \\neq P_{tst}(X)$$\n",
        "$$P_{tr}(y|X) = P_{tst}(y|X)$$\n",
        "\n",
        "Example: ùëã is medical data, ùë¶ is a¬†status of a disease. We test the¬†model at another hospital.\n",
        "\n",
        "In the case of the covariate shift, we need to be sure that our model is robust to these shifts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LardJzJor7jW"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/88453d7be9ef5bbb791f67df3697701639f5784c/images/shifts.png\" width=600 align='left'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfyOLOjOr7jX"
      },
      "source": [
        "This phenomenon of dataset shift in time is widespread across various domains:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USB0-0KTr7jX"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/data_description.png\" width=800 align=\"left\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QUzf9Kxr7jX"
      },
      "source": [
        "_Credits: Yao, Huaxiu, et al. \"Wild-time: A benchmark of in-the-wild distribution shift over time.\" Advances in Neural Information Processing Systems 35 (2022): 10309-10324._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F9DIud6r7jX"
      },
      "source": [
        "In this tutorial, we will learn how to evaluate the model robustness to covariate dataset shift and then will look at how the generative models can be used in the evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAr2QULdr7jX"
      },
      "source": [
        "## Dataset for time series forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awok3S-Nr7jX"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/airi-industrial-ai/dsc23-tutorial\n",
        "!pip install git+https://github.com/airi-industrial-ai/genrisk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNgp8SaHXs2y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange, tqdm\n",
        "import numpy as np\n",
        "import requests\n",
        "from scipy.stats import ks_2samp, norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.stattools import pacf, acf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6ONAT_rMv8d"
      },
      "outputs": [],
      "source": [
        "from genrisk.shift import ConditionalShift\n",
        "from genrisk.generation import TCNGAN, LSTMGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9umaWDJr7jZ"
      },
      "outputs": [],
      "source": [
        "from dsctutorial.data import load_gas_supply\n",
        "from dsctutorial.utils import positional_encoding, load_from_checkpoint\n",
        "from dsctutorial.plot import hist2samp, pacf2samp\n",
        "from dsctutorial.forecast import LSTMForecaster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y4oddP9r7ja"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhoLCWA6r7ja"
      },
      "source": [
        "The dataset is represented by weekly U.S. product supplied of finished motor gasoline (in thousand barrels per day). The dataset is available on the page https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=wgfupus2&f=W."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azykPx-TXnA1"
      },
      "outputs": [],
      "source": [
        "target = load_gas_supply()\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-TEFPGZcZJL"
      },
      "outputs": [],
      "source": [
        "split = target.index[int(len(target) * 0.7)]\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.vlines(x=split, ymin=target.min(), ymax=target.max(), color='tab:orange', label='train-test split')\n",
        "plt.plot(target)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SauO5g4yr7jb"
      },
      "source": [
        "Additionally, we consider some **covariates** which known in advance and help our model to predict the future. Here we will use the simple positional ecnoding of the week of year:\n",
        "$$\n",
        "\\text{pos. encoding}(i) = \\left(\\sin \\frac{i2\\pi}{52} , \\cos \\frac{i2\\pi}{52} \\right)\n",
        "$$\n",
        "where $i$ is the number of a week which is in the range from 0 to 52."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pGBeOT1bWMi"
      },
      "outputs": [],
      "source": [
        "cov = positional_encoding(target.index, ['weekofyear'])\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(cov)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDxToy1Kr7jb"
      },
      "source": [
        "Let us split our target and covariates into train and test samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhieU1T2YCDN"
      },
      "outputs": [],
      "source": [
        "train_target, test_target = target[:split], target[split:]\n",
        "scaler = StandardScaler()\n",
        "train_target[:] = scaler.fit_transform(train_target)\n",
        "test_target[:] = scaler.transform(test_target)\n",
        "train_cov, test_cov = cov[:split], cov[split:]\n",
        "len(train_target), len(test_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOB61Eezr7jc"
      },
      "source": [
        "Let us consider the train and test target data distribution. It is the common practice in time series data to compare the distribution of the difference between subsequent datapoints:\n",
        "$$\\text{diff}_i = x_i - x_{i-1}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW5KMPMcyYFh"
      },
      "outputs": [],
      "source": [
        "hist2samp(train_target, test_target, 'train', 'test', nbins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fhpOk51r7jc"
      },
      "source": [
        "We can see that some slight dataset shift is introduced in the test sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5TAuDzqr7jc"
      },
      "source": [
        "## LSTM forecasting model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9tPYRUbr7jc"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/lstm.png\" width=600 align=\"left\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlcpMDjEr7jc"
      },
      "source": [
        "Let us train a simple LSTM model to predict the target time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIol54S_MgW1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "lstm_model = LSTMForecaster(hidden_dim=64, window_size=100, lr=0.01, num_epochs=30, batch_size=32)\n",
        "lstm_model.fit(train_target, train_cov)\n",
        "\"\"\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGnDfoKCr7jd"
      },
      "source": [
        "Here we don't waste time for training a neural network, insted we download a pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xwKo7_Ar7jd"
      },
      "outputs": [],
      "source": [
        "url = 'https://github.com/airi-industrial-ai/dsc23-tutorial/raw/main/weights/lstm-epoch=14-step=450.ckpt'\n",
        "r = requests.get(url)\n",
        "open('lstm-epoch=14-step=450.ckpt', 'wb').write(r.content);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgiAuKQE7OxC",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "lstm_model = LSTMForecaster(hidden_dim=64, window_size=100, lr=0.01, num_epochs=15, batch_size=32)\n",
        "lstm_model.load_from_checkpoint(\n",
        "    'lstm-epoch=14-step=450.ckpt',\n",
        "    train_target,\n",
        "    train_cov,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8V51Gv2r7ji"
      },
      "source": [
        "Let us look how the model predict the future on test sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr915g7bZYXF"
      },
      "outputs": [],
      "source": [
        "pred = lstm_model.predict(len(test_target), train_target, train_cov, test_cov)\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(test_target, label='test target')\n",
        "plt.plot(pred, label='forecast')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5njfD-RYr7ji"
      },
      "source": [
        "## Backtesting of forecasting models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZJnjGb8r7ji"
      },
      "source": [
        "Making prediction for 9 years ahead forecasting is nonsense. Instead, the common practice in testing forecasting models is **backtesting**. Backtesting is the procedure which evaluates the model in the historical order using the sliding window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkYZSowPr7ji"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/sliding_window.png\" width=800 align='left'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmHbWyUTr7ji"
      },
      "source": [
        "We split each window into past range and future range with respect to input size and forecasting horizon. We will use the past target, past covariates and future covariates to forecast the future target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx0cuoFbfIIy"
      },
      "outputs": [],
      "source": [
        "def backtest(model, input_size, horizon, target, cov):\n",
        "    error = []\n",
        "    for i in trange(len(target)-input_size-horizon+1):\n",
        "        past_range = range(i, i+input_size)\n",
        "        future_range = range(i+input_size, i+input_size+horizon)\n",
        "        past_target = target.iloc[past_range]\n",
        "        past_cov = cov.iloc[past_range]\n",
        "        future_cov = cov.iloc[future_range]\n",
        "        future_target = target.iloc[future_range]\n",
        "        future_pred = model.predict(horizon, past_target, past_cov, future_cov)\n",
        "        error.append(((future_target.values[-1] - future_pred.values[-1])**2).mean())\n",
        "    return pd.Series(error, index=target.index[input_size-1:-horizon])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRlZePmJr7ji"
      },
      "source": [
        "We will use the followin forecasting problem:\n",
        "* input size is 10 weeks\n",
        "* forecasting horizon is 100 weeks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBG0cdKqcgl1"
      },
      "outputs": [],
      "source": [
        "train_lstm_error = backtest(lstm_model, input_size=10, horizon=100, target=train_target, cov=train_cov)\n",
        "test_lstm_error = backtest(lstm_model, input_size=10, horizon=100, target=test_target, cov=test_cov)\n",
        "train_lstm_error.mean(), test_lstm_error.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwdojO8Tr7jj"
      },
      "source": [
        "## How to estimate the possible risks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40jQKgdHr7jj"
      },
      "source": [
        "**Idea: some variables can change, other are fixed.**\n",
        "\n",
        "* __Immutable variables__ keep their distribution unchanged while dataset shifts occur (e.g. demographic characteristics)\n",
        "* __Mutable variables__ can have changes (e.g. treatment plan)\n",
        "\n",
        "Notation:\n",
        "* $X = W \\cup Z$\n",
        "* $W$ ‚Äî mutable variables\n",
        "* $Z$ ‚Äî immutable variables\n",
        "\n",
        "Original data: $P_{tr}(X) = P(W|Z)P(Z)$\n",
        "\n",
        "Shifted data: $P_{tst}(X)=Q(W|Z)P(Z)$\n",
        "\n",
        "**Worst-case risk** is the method to esimate potential decrease of the quality due to dataset shift. The steps of estimation:\n",
        "1. Find the small fraction of the data which the model predicts the worst\n",
        "2. Make sure the distribution of immutable variables is (almost) the same\n",
        "3. Evaluate the model on this subsample\n",
        "\n",
        "**The smaller the fraction, the higher the risk and vice versa.** The size of the fraction is denotes as $(1-\\alpha)$.\n",
        "\n",
        "To do these steps we use two models:\n",
        "1. $\\mu$: __Expectation model__ takes both mutable and immutable variables and estimate the expected loss of the target model\n",
        "2. $\\eta$: __Quantile model__ takes immutable variables and estimate the $\\alpha$-quantile of the loss of the target model\n",
        "3. $x$ is in the worst subsample if $\\mu(w, z) > \\eta(z)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXLQlqhDr7jj"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/wcr.png\" width=800 align='left'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKB_z3-Nr7jj"
      },
      "source": [
        "_Credits: Subbaswamy, Adarsh, Roy Adams, and Suchi Saria. \"Evaluating model\n",
        "    robustness and stability to dataset shift.\" International Conference on Artificial\n",
        "    Intelligence and Statistics. PMLR, 2021._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFJRbCFr7jj"
      },
      "source": [
        "Let us create a dataset which we use to define mutable and immutable variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l75jmHdswerj"
      },
      "outputs": [],
      "source": [
        "def input_seq_dataset(input_size, horizon, target, cov, lags):\n",
        "    input_seq = []\n",
        "    for i in trange(len(target)-input_size-horizon+1):\n",
        "        past_range = range(i, i+input_size)\n",
        "        future_range = range(i+input_size, i+input_size+horizon)\n",
        "        past_target = target.iloc[past_range]\n",
        "        past_cov = cov.iloc[past_range]\n",
        "        past_target_lags = []\n",
        "        for j in range(lags):\n",
        "            past_target_lags.append(past_target.iloc[-1, 0] - past_target.iloc[-2-j, 0])\n",
        "        input = np.concatenate([past_target_lags, past_cov.iloc[-1].values])\n",
        "        input_seq.append(input)\n",
        "    columns = [f'{target.columns[0]}_diff{i+1}' for i in range(lags)] + list(cov.columns)\n",
        "    input_seq = pd.DataFrame(input_seq, columns=columns, index=target.index[input_size-1:-horizon])\n",
        "    return input_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZzMXwRUNy-o"
      },
      "outputs": [],
      "source": [
        "test_input_seq = input_seq_dataset(\n",
        "    input_size=10, horizon=100, target=test_target, cov=test_cov, lags=5)\n",
        "test_input_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDtLn9ALr7jk"
      },
      "source": [
        "Let the variables with diffs be mutable and weekofyear cos and sin be immutable variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNMxCqvLL6Is"
      },
      "outputs": [],
      "source": [
        "mutable_columns = test_input_seq.columns[:-2].tolist()\n",
        "immutable_columns = test_input_seq.columns[-2:].tolist()\n",
        "mutable_columns, immutable_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAiYbWwFr7jk"
      },
      "source": [
        "We estimate the worst-case risk on different levels of risk: from 0.1 to 0.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91kkz00TLz-J"
      },
      "outputs": [],
      "source": [
        "alphaspace = np.linspace(0.1, 0.9, 9)\n",
        "test_lstm_risk = []\n",
        "for alpha in tqdm(alphaspace):\n",
        "    shift_model = ConditionalShift(mutable_columns, immutable_columns, alpha=alpha)\n",
        "    shift_model.fit(test_input_seq, test_lstm_error)\n",
        "    test_lstm_risk.append((shift_model.risk, shift_model.lb_risk, shift_model.ub_risk))\n",
        "test_lstm_risk = np.array(test_lstm_risk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGNrM9Qtr7jk"
      },
      "source": [
        "We plot risk values and 95% confidence interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asW7urHGRCqJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(alphaspace, test_lstm_risk[:, 0], c='tab:green')\n",
        "plt.fill_between(alphaspace, test_lstm_risk[:, 1], test_lstm_risk[:, 2], alpha=0.2, color='tab:green')\n",
        "plt.hlines(test_lstm_error.mean(), alphaspace[0], alphaspace[-1], color='tab:blue', label='mean error')\n",
        "plt.legend()\n",
        "plt.title('Worst-case risk evaluation')\n",
        "plt.xlabel('alpha, risk level')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcCdeIymr7jk"
      },
      "source": [
        "The model stores the mask for the worst subsample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx5BmHJzr7jl"
      },
      "outputs": [],
      "source": [
        "shift_model.mask[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQbKbhucr7jl"
      },
      "source": [
        "Let us plot some example from the worst subsample (stress data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wcw51n_r7jl"
      },
      "outputs": [],
      "source": [
        "risk_date = test_input_seq[shift_model.mask].index[0]\n",
        "risk_idx = np.where(test_target.index == risk_date)[0][0]\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(test_target.iloc[risk_idx-10:risk_idx+100])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCYGtjK3r7jl"
      },
      "source": [
        "**Limitations** of the worst-case risk:\n",
        "* High uncertainty in evaluation of risk with high $\\alpha$\n",
        "* Not all possible shifts are presented\n",
        "\n",
        "The suggestion: let us generate new data to model covariate shifts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhKVPAgnr7jl"
      },
      "source": [
        "## Time series generation using GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVvLuJYTr7jl"
      },
      "source": [
        "The difference between generative and discriminative models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezVTBIYwr7jl"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/gen_vs_disc.png' width=800 align='left'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcZN0E0Er7jl"
      },
      "source": [
        "*Credits: https://medium.com/@jordi299/about-generative-and-discriminative-models-d8958b67ad32*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGyrj0URr7jl"
      },
      "source": [
        "| Discriminative models | Generative models |\n",
        "|:---|:---|\n",
        "| Aims to predict the label by the given example| Aims to learn the full data distribution by the given subsample |\n",
        "| Learns the probability $y \\sim P(y|X)$ | Learns the probability $X,y \\sim P(X, y)$ or $X \\sim P(X)$ |\n",
        "| _Examples of tasks:_ | _Examples of tasks:_ |\n",
        "| 1. Is a cat on the photo? | 1. Create a photo of a cat that looks like the average of two given cat photos |\n",
        "| 2. How old is a person on the photo? | 2. Create a photo of an avocado armchair |\n",
        "| 3. What is the most likely number of tourists the next mounth? | 3. Write an essay on the topic of global warming |\n",
        "| _Examples of models:_ | _Examples of models:_ |\n",
        "| Decision Tree, Support Vector Machine (SVM), Convolutional Neural Network (CNN) | Autoregressive model (AR), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), Generative Pretrained Transformer (GPT)|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXoUcfsFr7jl"
      },
      "source": [
        "Deep generative models are based on deep neural networks.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/space_generator.png' width=800 align='left'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pl8YQC5r7jm"
      },
      "source": [
        "Generative Adversarial Network is a generative model that can be used to generate time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94CoiSdSr7jm"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/gan_for_ts.png' width=800 align='left'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DwzTOLfr7jm"
      },
      "source": [
        "A Generative Adversarial Network (GAN) is a type of model that consists of two neural networks: a generator and a discriminator.\n",
        "\n",
        "* The objective of a generator (G) is to create realistic data\n",
        "* The objective of a discriminator (D) is to detect fake data\n",
        "\n",
        "We note the multivariate time series as $X_1, X_2, \\dots, X_N$, where $X_t \\in R^d$\n",
        "\n",
        "The general objective of GAN is\n",
        "\n",
        "$$\\min_G \\max_D V(D, G) = \\mathbb E_{X} \\log(D(X)) + \\mathbb E_{Z} \\log(1 - D(G(Z))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdYpyQ2nr7jm"
      },
      "source": [
        "In this tutorial we use GAN where a generator and a discriminator are Temporal Convolutional Networks (TCN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m870kVOTr7jm"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/tcn.png' width=700 align='left'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwLGD_Mmr7jm"
      },
      "source": [
        "_Credits: Oord, Aaron van den, et al. \"Wavenet: A generative model for raw audio.\" arXiv preprint arXiv:1609.03499 (2016)._\n",
        "\n",
        "The main advantages of TCN:\n",
        "* Exponentially large receptive field ‚Äî TCN can detect long and short patterns\n",
        "* Can be applied to time series of arbitrary length\n",
        "* Causal convolution prevents leakage from the future\n",
        "\n",
        "The key parameter of TCN is the number of channels (also known as filters, kernels).\n",
        "\n",
        "For generator:\n",
        "* Input channels: latent space dimensionality\n",
        "* Hidden channels: some large number to model temporal relationships\n",
        "* Output channels: time series dimensionality\n",
        "\n",
        "For discriminator:\n",
        "* Input channels: time series dimensionality\n",
        "* Hidden channels: some large number to model temporal relationships\n",
        "* Output channels: 1 (the probability that the data is fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpVx4z3DCn5a"
      },
      "outputs": [],
      "source": [
        "train_data = pd.concat([train_target, train_cov], axis=1)\n",
        "test_data = pd.concat([test_target, test_cov], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOxrLp-mr7jm"
      },
      "source": [
        "Let us define our TCN GAN:\n",
        "* `latent_dim` ‚Äî the dimensionality of latent space which is Miltivariate Normal Distribution.\n",
        "* `hidden_dim` ‚Äî the number of channels/filters/kernels in the hidden layers\n",
        "* `target_dim` ‚Äî the dimenstionality of time series\n",
        "* `num_layers` ‚Äî the number of TCN layers\n",
        "* `lr` ‚Äî the learning rate for the optimization step\n",
        "\n",
        "The values of parameters can be tuned using some search strategies, such as grid or random search.\n",
        "\n",
        "Now we can start to train GAN. We will use test data for training GAN as we are looking for shifts near the test period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5lo0ojAer68"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "gan = TCNGAN(\n",
        "    target_columns=target.columns,\n",
        "    conditional_columns=cov.columns,\n",
        "    window_size=100,\n",
        "    num_epochs=400,\n",
        "    num_layers=1,\n",
        "    hidden_dim=64,\n",
        "    latent_dim=2,\n",
        "    verbose=True,\n",
        "    lr=0.01,\n",
        ")\n",
        "gan.fit(test_data)\n",
        "\"\"\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrwQeWz4r7jn"
      },
      "source": [
        "GAN is known as an unstable model in training, since the generator and discriminator fluctuate without a guarantee of convergence. Thus, it is strongly recommended to monitor auxiliary metrics to determine the total number of training steps, e.g. KS-distance or EMD.\n",
        "\n",
        "Here we load the pretrained model from a `checkpoint` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztAwHrwYr7jn"
      },
      "outputs": [],
      "source": [
        "url = 'https://github.com/airi-industrial-ai/dsc23-tutorial/raw/main/weights/gan-epoch=399-step=26000.ckpt'\n",
        "r = requests.get(url)\n",
        "open('gan-epoch=399-step=26000.ckpt', 'wb').write(r.content);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4d4UZviCdpU"
      },
      "outputs": [],
      "source": [
        "gan = TCNGAN(\n",
        "    target_columns=target.columns,\n",
        "    conditional_columns=cov.columns,\n",
        "    window_size=100,\n",
        "    num_epochs=400,\n",
        "    num_layers=1,\n",
        "    hidden_dim=64,\n",
        "    latent_dim=2,\n",
        "    verbose=True,\n",
        "    lr=0.01,\n",
        ")\n",
        "load_from_checkpoint(gan, 'gan-epoch=399-step=26000.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLlAP14Zr7jn"
      },
      "source": [
        "## Evaluation of generated time series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu8xdjcBr7jn"
      },
      "source": [
        "Visual evaluation by time series plots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sRQpQKdhxwT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 3))\n",
        "fakes = gan.sample(test_data, n_samples=5)\n",
        "for fake in fakes:\n",
        "    fake_target = fake[target.columns]\n",
        "    fake_cov = fake[cov.columns]\n",
        "    plt.plot(fake_target, alpha=0.5)\n",
        "plt.plot(test_target, c='black')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AY8CYPAr7jn"
      },
      "source": [
        "Visual evaluation by histograms of difference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX5Y3iDYKTaf"
      },
      "outputs": [],
      "source": [
        "hist2samp(test_target, fake_target, 'test', 'fake', nbins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwUB36tzr7jo"
      },
      "source": [
        "Visual evaluation by auto-correlation functions of time series:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0hPQRLQURlp"
      },
      "outputs": [],
      "source": [
        "pacf2samp(test_target, fake_target, 'test', 'fake', nlags=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQmY-W9Or7jo"
      },
      "source": [
        "Statistical testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihv0n1n6r7jo"
      },
      "outputs": [],
      "source": [
        "ks_2samp(\n",
        "    (fake_target - fake_target.shift(1)).supply,\n",
        "    (test_target - test_target.shift(1)).supply\n",
        ").pvalue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIkCh0c3r7jo"
      },
      "source": [
        "Comparison with baselines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pP1ovobr7jo"
      },
      "outputs": [],
      "source": [
        "def pacf_error(target0, target1, nlags=10):\n",
        "    pacf0 = acf(target0, nlags=nlags)[1:]\n",
        "    pacf1 = acf(target1, nlags=nlags)[1:]\n",
        "    error = (pacf0 - pacf1)**2\n",
        "    return error.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH4mZDfSr7jo"
      },
      "outputs": [],
      "source": [
        "pacf_error(test_target, fake_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdBThL5Hr7jo"
      },
      "outputs": [],
      "source": [
        "noise = np.random.randn(len(test_target))\n",
        "pacf_error(test_target, noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n6l9Hbdr7jo"
      },
      "source": [
        "We can see that the fake time series are similar to real ones. For more information, see, for example, Wiese, Magnus, et al. \"Quant GANs: deep generation of financial time series.\" Quantitative Finance 20.9 (2020): 1419-1440.\n",
        "\n",
        "Conclusions so far:\n",
        "* We split the dataset into train, test sets\n",
        "* We trained GAN on the test set, as we are looking for shifts close to the test period\n",
        "* We created fake time series and visually checked that they are similar to the real ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrkRiMQ6r7jp"
      },
      "source": [
        "## Worst-case risk on fake data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qapjSEzKr7jp"
      },
      "source": [
        "Generative models has generalization property: they can generate plausible examples that are invisible in real data. We can use it to find new covariate shifts in the original dsitribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27JCtVUOr7jp"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/airi-industrial-ai/dsc23-tutorial/8e761b9a2851a923970afb548275aebffc480a55/images/wrc_fake.png' width=700 align='left'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6y85LJHr7jp"
      },
      "source": [
        "Let us generate data using GAN and backtest the model on fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSxa_fleDG_V"
      },
      "outputs": [],
      "source": [
        "fake_index = pd.date_range(\n",
        "    start=test_target.index[0],\n",
        "    periods=len(test_target)*5,\n",
        "    freq=test_target.index.freq\n",
        ")\n",
        "fake_cov = positional_encoding(fake_index, ['weekofyear'])\n",
        "fake_target = gan.sample(fake_cov, n_samples=1)[0][target.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtvQsqR9JKKx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fake_lstm_error = backtest(lstm_model, input_size=10, horizon=100, target=fake_target, cov=fake_cov)\n",
        "fake_lstm_error.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t2-YXqvr7jp"
      },
      "source": [
        "As we see, the quality on the fake set is close to the test set. It means that the model cannot detect much discrepancy between them.\n",
        "\n",
        "We define mutable and immutable variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAfz2c5VKgh1"
      },
      "outputs": [],
      "source": [
        "fake_input_seq = input_seq_dataset(\n",
        "    input_size=10, horizon=100,\n",
        "    target=fake_target,\n",
        "    cov=fake_cov,\n",
        "    lags=5)\n",
        "fake_input_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9vjYulLr7jq"
      },
      "source": [
        "We estimate worst-case risk on fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gmZORPk6VXR"
      },
      "outputs": [],
      "source": [
        "alphaspace = np.linspace(0.1, 0.9, 9)\n",
        "fake_lstm_risk = []\n",
        "for alpha in tqdm(alphaspace):\n",
        "    shift_model = ConditionalShift(mutable_columns, immutable_columns, alpha=alpha)\n",
        "    shift_model.fit(fake_input_seq, fake_lstm_error)\n",
        "    fake_lstm_risk.append((shift_model.risk, shift_model.lb_risk, shift_model.ub_risk))\n",
        "fake_lstm_risk = np.array(fake_lstm_risk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a91lEAtqr7jq"
      },
      "source": [
        "Let us compare estimated risk on test and fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLoZKJRCNhJJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(alphaspace, test_lstm_risk[:, 0], c='tab:blue', label='test data')\n",
        "plt.fill_between(alphaspace, test_lstm_risk[:, 1], test_lstm_risk[:, 2], alpha=0.2, color='tab:blue')\n",
        "\n",
        "plt.plot(alphaspace, fake_lstm_risk[:, 0], c='tab:green', label='fake data')\n",
        "plt.fill_between(alphaspace, fake_lstm_risk[:, 1], fake_lstm_risk[:, 2], alpha=0.2, color='tab:green')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Worst-case risk evaluation')\n",
        "plt.xlabel('alpha, risk level')\n",
        "plt.ylabel('MSE')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHLL6I-nr7jq"
      },
      "source": [
        "As we can see, we can get a narrower uncertainty interval for fake data. This means that we can more precisely assess the robustness of the model using fake data.\n",
        "\n",
        "Finally, let us look at some generated stress examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJzl0vfvr7jq"
      },
      "outputs": [],
      "source": [
        "risk_date = fake_input_seq[shift_model.mask].index[0]\n",
        "risk_idx = np.where(fake_target.index == risk_date)[0][0]\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(test_target.iloc[risk_idx-10:risk_idx+100])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5xVRZEbr7jq"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Qrs69Jr7jq"
      },
      "source": [
        "Generative models produce realistic data that can help estimate the model robustness to dataset shifts, however it brings some challenges.\n",
        "\n",
        "1. It is hard to tune parameters and train deep generative models such GAN\n",
        "2. It is hard to estimate how close the fake time series to real ones: no scientific standard so far\n",
        "3. We assume that generative model generalizes original data distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DhGYgaxr7jr"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybg8scOAr7jr"
      },
      "source": [
        "Today we've learned the following:\n",
        "\n",
        "1. What is the dataset shifts\n",
        "2. How to prepare a dataset for time series forecasting\n",
        "3. How to train and test a LSTM-based forecasting model\n",
        "4. What is the worst-case risk\n",
        "5. How to evaluate the stability of the LSTM-based model using the worst-case risk\n",
        "6. What is the time series generation\n",
        "7. How to generate fake time series similar to the original dataset\n",
        "8. How to evaluate the stablity of the LSTM-based model using the worst-case risk on fake data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl05MvYHr7jr"
      },
      "source": [
        "### Feel free to write me on pozdnyakov@airi.net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCItXYzKr7jr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}